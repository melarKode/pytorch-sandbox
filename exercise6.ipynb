{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_path = \"C:/Navneet/College/datascience/pytorch/dlwpt-code/data/p1ch4/tabular-wine/winequality-white.csv\"\n",
    "wineq_numpy = np.loadtxt(wine_path, delimiter=\";\", skiprows=1)\n",
    "wineq_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-00f132b8bb0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "col_list = next(csv.reader(open(wine_path, \"r\"), delimiter=\";\"))\n",
    "len(col_list)\n",
    "\n",
    "output_tensor = torch.tensor(col_list)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = wineq_numpy[:,:-1]\n",
    "output_data = wineq_numpy[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0000,  0.2700,  ...,  0.4500,  8.8000],\n",
       "        [ 6.3000,  0.3000,  ...,  0.4900,  9.5000],\n",
       "        ...,\n",
       "        [ 5.5000,  0.2900,  ...,  0.3800, 12.8000],\n",
       "        [ 6.0000,  0.2100,  ...,  0.3200, 11.8000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq_tensor = torch.tensor(input_data, dtype=torch.float)\n",
    "wineq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3919]), torch.Size([979]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = wineq_tensor.shape[0]\n",
    "n_val = int(n_samples*0.2)\n",
    "\n",
    "shuffle = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffle[:-n_val]\n",
    "val_indices = shuffle[-n_val:]\n",
    "\n",
    "train_indices.shape,val_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.,  ..., 0., 0.],\n",
       "        [0., 0.,  ..., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0.,  ..., 0., 0.],\n",
       "        [0., 0.,  ..., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = torch.zeros(n_samples,10)\n",
    "output_tensor.scatter_(1, torch.tensor(output_data, dtype=torch.int64).unsqueeze_(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), 6.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor[0], output_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq_train = wineq_tensor[train_indices]\n",
    "output_train = output_tensor[train_indices]\n",
    "wineq_val = wineq_tensor[val_indices]\n",
    "output_val = output_tensor[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden layer): Linear(in_features=11, out_features=10, bias=True)\n",
       "  (hidden activation): Softmax(dim=1)\n",
       "  (output layer): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    ('hidden layer', nn.Linear(11, 10)),\n",
    "    ('hidden activation', nn.Softmax(1)),\n",
    "    ('output layer', nn.Linear(10, 10)),\n",
    "]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer.weight torch.Size([10, 11])\n",
      "hidden layer.bias torch.Size([10])\n",
      "output layer.weight torch.Size([10, 10])\n",
      "output layer.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs,model,optimizer,loss_fn,inp_train,out_train,inp_val,out_val):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        pred_train = model(inp_train)\n",
    "        pred_loss = loss_fn(pred_train,out_train)\n",
    "\n",
    "        pred_val = model(inp_val)\n",
    "        pred_val_loss = loss_fn(pred_val,out_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch==1 or epoch % 10 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Train loss:\", pred_loss.item(), \"Val loss:\", pred_val_loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.2445429563522339 Val loss: 1.2496105432510376\n",
      "Epoch: 10 Train loss: 1.2432286739349365 Val loss: 1.2483487129211426\n",
      "Epoch: 20 Train loss: 1.2417892217636108 Val loss: 1.246720790863037\n",
      "Epoch: 30 Train loss: 1.240376591682434 Val loss: 1.2451210021972656\n",
      "Epoch: 40 Train loss: 1.238988995552063 Val loss: 1.2435845136642456\n",
      "Epoch: 50 Train loss: 1.2376246452331543 Val loss: 1.2420862913131714\n",
      "Epoch: 60 Train loss: 1.2362834215164185 Val loss: 1.2406145334243774\n",
      "Epoch: 70 Train loss: 1.2349646091461182 Val loss: 1.239159345626831\n",
      "Epoch: 80 Train loss: 1.2336676120758057 Val loss: 1.237716555595398\n",
      "Epoch: 90 Train loss: 1.2323919534683228 Val loss: 1.2363086938858032\n",
      "Epoch: 100 Train loss: 1.2311370372772217 Val loss: 1.234913945198059\n",
      "Epoch: 110 Train loss: 1.2299026250839233 Val loss: 1.2335399389266968\n",
      "Epoch: 120 Train loss: 1.2286921739578247 Val loss: 1.232116937637329\n",
      "Epoch: 130 Train loss: 1.227510690689087 Val loss: 1.231060266494751\n",
      "Epoch: 140 Train loss: 1.226330280303955 Val loss: 1.229707956314087\n",
      "Epoch: 150 Train loss: 1.2251678705215454 Val loss: 1.22835111618042\n",
      "Epoch: 160 Train loss: 1.22402822971344 Val loss: 1.227034568786621\n",
      "Epoch: 170 Train loss: 1.2229087352752686 Val loss: 1.225771427154541\n",
      "Epoch: 180 Train loss: 1.2218071222305298 Val loss: 1.224542498588562\n",
      "Epoch: 190 Train loss: 1.2207223176956177 Val loss: 1.2233307361602783\n",
      "Epoch: 200 Train loss: 1.219653844833374 Val loss: 1.2221252918243408\n",
      "Epoch: 210 Train loss: 1.218601942062378 Val loss: 1.2209447622299194\n",
      "Epoch: 220 Train loss: 1.217565655708313 Val loss: 1.2197892665863037\n",
      "Epoch: 230 Train loss: 1.2165452241897583 Val loss: 1.2186459302902222\n",
      "Epoch: 240 Train loss: 1.2155394554138184 Val loss: 1.2175215482711792\n",
      "Epoch: 250 Train loss: 1.2145543098449707 Val loss: 1.2164982557296753\n",
      "Epoch: 260 Train loss: 1.2135908603668213 Val loss: 1.2151716947555542\n",
      "Epoch: 270 Train loss: 1.2125442028045654 Val loss: 1.2140579223632812\n",
      "Epoch: 280 Train loss: 1.2113267183303833 Val loss: 1.2126985788345337\n",
      "Epoch: 290 Train loss: 1.2099175453186035 Val loss: 1.211012363433838\n",
      "Epoch: 300 Train loss: 1.2085340023040771 Val loss: 1.2087758779525757\n",
      "Epoch: 310 Train loss: 1.2069612741470337 Val loss: 1.2051351070404053\n",
      "Epoch: 320 Train loss: 1.2049423456192017 Val loss: 1.2009339332580566\n",
      "Epoch: 330 Train loss: 1.202487826347351 Val loss: 1.1972795724868774\n",
      "Epoch: 340 Train loss: 1.2000762224197388 Val loss: 1.1940479278564453\n",
      "Epoch: 350 Train loss: 1.1979411840438843 Val loss: 1.191530704498291\n",
      "Epoch: 360 Train loss: 1.1959824562072754 Val loss: 1.1893410682678223\n",
      "Epoch: 370 Train loss: 1.194170594215393 Val loss: 1.1874380111694336\n",
      "Epoch: 380 Train loss: 1.1924511194229126 Val loss: 1.1858428716659546\n",
      "Epoch: 390 Train loss: 1.190792202949524 Val loss: 1.1844127178192139\n",
      "Epoch: 400 Train loss: 1.1891790628433228 Val loss: 1.1830999851226807\n",
      "Epoch: 410 Train loss: 1.1876099109649658 Val loss: 1.181833267211914\n",
      "Epoch: 420 Train loss: 1.1860896348953247 Val loss: 1.1805757284164429\n",
      "Epoch: 430 Train loss: 1.1846208572387695 Val loss: 1.1793268918991089\n",
      "Epoch: 440 Train loss: 1.1832791566848755 Val loss: 1.1780414581298828\n",
      "Epoch: 450 Train loss: 1.1818569898605347 Val loss: 1.176882028579712\n",
      "Epoch: 460 Train loss: 1.1805293560028076 Val loss: 1.1757348775863647\n",
      "Epoch: 470 Train loss: 1.1792429685592651 Val loss: 1.1746400594711304\n",
      "Epoch: 480 Train loss: 1.178003191947937 Val loss: 1.173537015914917\n",
      "Epoch: 490 Train loss: 1.1767939329147339 Val loss: 1.1724244356155396\n",
      "Epoch: 500 Train loss: 1.1756134033203125 Val loss: 1.1713225841522217\n",
      "Epoch: 510 Train loss: 1.1744588613510132 Val loss: 1.1702311038970947\n",
      "Epoch: 520 Train loss: 1.1733283996582031 Val loss: 1.1691478490829468\n",
      "Epoch: 530 Train loss: 1.1722216606140137 Val loss: 1.1680715084075928\n",
      "Epoch: 540 Train loss: 1.1711399555206299 Val loss: 1.167004108428955\n",
      "Epoch: 550 Train loss: 1.1700845956802368 Val loss: 1.1659482717514038\n",
      "Epoch: 560 Train loss: 1.1690552234649658 Val loss: 1.1648987531661987\n",
      "Epoch: 570 Train loss: 1.168050765991211 Val loss: 1.1638561487197876\n",
      "Epoch: 580 Train loss: 1.167069435119629 Val loss: 1.162818431854248\n",
      "Epoch: 590 Train loss: 1.1661118268966675 Val loss: 1.161799430847168\n",
      "Epoch: 600 Train loss: 1.1651796102523804 Val loss: 1.1607927083969116\n",
      "Epoch: 610 Train loss: 1.164267659187317 Val loss: 1.159747838973999\n",
      "Epoch: 620 Train loss: 1.1633762121200562 Val loss: 1.1587532758712769\n",
      "Epoch: 630 Train loss: 1.1624903678894043 Val loss: 1.1577801704406738\n",
      "Epoch: 640 Train loss: 1.1616308689117432 Val loss: 1.1568094491958618\n",
      "Epoch: 650 Train loss: 1.1607850790023804 Val loss: 1.1558374166488647\n",
      "Epoch: 660 Train loss: 1.1599535942077637 Val loss: 1.154870867729187\n",
      "Epoch: 670 Train loss: 1.1591355800628662 Val loss: 1.1539092063903809\n",
      "Epoch: 680 Train loss: 1.1583303213119507 Val loss: 1.1529537439346313\n",
      "Epoch: 690 Train loss: 1.1575371026992798 Val loss: 1.1520051956176758\n",
      "Epoch: 700 Train loss: 1.1567548513412476 Val loss: 1.151060700416565\n",
      "Epoch: 710 Train loss: 1.1559830904006958 Val loss: 1.1501209735870361\n",
      "Epoch: 720 Train loss: 1.155221700668335 Val loss: 1.1491873264312744\n",
      "Epoch: 730 Train loss: 1.1545946598052979 Val loss: 1.1484220027923584\n",
      "Epoch: 740 Train loss: 1.1537634134292603 Val loss: 1.1473894119262695\n",
      "Epoch: 750 Train loss: 1.153012990951538 Val loss: 1.1464470624923706\n",
      "Epoch: 760 Train loss: 1.1522785425186157 Val loss: 1.1455214023590088\n",
      "Epoch: 770 Train loss: 1.1515591144561768 Val loss: 1.1446083784103394\n",
      "Epoch: 780 Train loss: 1.1508461236953735 Val loss: 1.1436963081359863\n",
      "Epoch: 790 Train loss: 1.1501368284225464 Val loss: 1.1427807807922363\n",
      "Epoch: 800 Train loss: 1.1494306325912476 Val loss: 1.1418609619140625\n",
      "Epoch: 810 Train loss: 1.1487261056900024 Val loss: 1.1409367322921753\n",
      "Epoch: 820 Train loss: 1.1480218172073364 Val loss: 1.1400072574615479\n",
      "Epoch: 830 Train loss: 1.1473171710968018 Val loss: 1.1390736103057861\n",
      "Epoch: 840 Train loss: 1.146807312965393 Val loss: 1.1383625268936157\n",
      "Epoch: 850 Train loss: 1.1459406614303589 Val loss: 1.1372779607772827\n",
      "Epoch: 860 Train loss: 1.1452274322509766 Val loss: 1.1363486051559448\n",
      "Epoch: 870 Train loss: 1.144511103630066 Val loss: 1.1354435682296753\n",
      "Epoch: 880 Train loss: 1.1438065767288208 Val loss: 1.1345634460449219\n",
      "Epoch: 890 Train loss: 1.143100380897522 Val loss: 1.1336878538131714\n",
      "Epoch: 900 Train loss: 1.142393708229065 Val loss: 1.1328258514404297\n",
      "Epoch: 910 Train loss: 1.1416878700256348 Val loss: 1.131981611251831\n",
      "Epoch: 920 Train loss: 1.140984296798706 Val loss: 1.1311525106430054\n",
      "Epoch: 930 Train loss: 1.140282154083252 Val loss: 1.1303369998931885\n",
      "Epoch: 940 Train loss: 1.1395748853683472 Val loss: 1.1295311450958252\n",
      "Epoch: 950 Train loss: 1.1387847661972046 Val loss: 1.1288589239120483\n",
      "Epoch: 960 Train loss: 1.1373146772384644 Val loss: 1.13141667842865\n",
      "Epoch: 970 Train loss: 1.1361231803894043 Val loss: 1.1289931535720825\n",
      "Epoch: 980 Train loss: 1.1352206468582153 Val loss: 1.1281150579452515\n",
      "Epoch: 990 Train loss: 1.1344242095947266 Val loss: 1.1277670860290527\n",
      "Epoch: 1000 Train loss: 1.1336413621902466 Val loss: 1.12699556350708\n"
     ]
    }
   ],
   "source": [
    "finalmodel = training_loop(1000,model,optimizer,nn.CrossEntropyLoss(),wineq_train,output_train,wineq_val,output_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(output_data[train_indices],finalmodel(wineq_train).argmax(1).unsqueeze(1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37949852516ccaa153c0024f6d4d7ac0036053c674905739b9a34c73111dd78b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

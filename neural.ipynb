{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1,28,28,device=device)\n",
    "logits = model(X)\n",
    "pred_prob = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_prob.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6041, 0.5135, 0.1469,  ..., 0.0820, 0.1605, 0.6547],\n",
       "         [0.1467, 0.3011, 0.4751,  ..., 0.9297, 0.4914, 0.3636],\n",
       "         [0.2215, 0.0823, 0.9004,  ..., 0.0977, 0.4430, 0.7759],\n",
       "         ...,\n",
       "         [0.5786, 0.3320, 0.8674,  ..., 0.8641, 0.9467, 0.6140],\n",
       "         [0.4422, 0.0724, 0.3196,  ..., 0.2281, 0.0018, 0.3934],\n",
       "         [0.7731, 0.6703, 0.5960,  ..., 0.5554, 0.2984, 0.4795]],\n",
       "\n",
       "        [[0.4366, 0.7917, 0.2892,  ..., 0.3705, 0.4066, 0.6367],\n",
       "         [0.2909, 0.7673, 0.5162,  ..., 0.8016, 0.0377, 0.0178],\n",
       "         [0.9699, 0.2774, 0.4014,  ..., 0.2846, 0.2824, 0.0261],\n",
       "         ...,\n",
       "         [0.8731, 0.5381, 0.9999,  ..., 0.8456, 0.6700, 0.7751],\n",
       "         [0.2115, 0.9370, 0.4110,  ..., 0.8305, 0.0896, 0.2563],\n",
       "         [0.1222, 0.6142, 0.4756,  ..., 0.5201, 0.6055, 0.4720]],\n",
       "\n",
       "        [[0.8664, 0.2934, 0.4921,  ..., 0.4675, 0.5531, 0.7664],\n",
       "         [0.2449, 0.2689, 0.5860,  ..., 0.7101, 0.7451, 0.5257],\n",
       "         [0.4963, 0.6205, 0.2995,  ..., 0.8845, 0.3388, 0.4782],\n",
       "         ...,\n",
       "         [0.8762, 0.6162, 0.0370,  ..., 0.7049, 0.8653, 0.0852],\n",
       "         [0.5768, 0.6237, 0.2338,  ..., 0.8142, 0.1536, 0.4573],\n",
       "         [0.9657, 0.5439, 0.9379,  ..., 0.9957, 0.3120, 0.2940]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28,out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.5655,  0.1406, -0.3301,  0.4963,  0.2198,  0.6241,  0.2940,  0.8591,\n",
      "          0.5996, -0.2409,  0.2359, -0.2091, -0.2553, -0.2885,  0.0269,  0.0130,\n",
      "         -0.3040,  0.1911, -0.1305,  0.4135],\n",
      "        [ 0.6640,  0.2588, -0.4565,  0.5884,  0.6104,  0.2786,  0.0273,  0.9439,\n",
      "          0.5719, -0.4307,  0.1015,  0.2043, -0.3149, -0.5887,  0.1580, -0.1218,\n",
      "         -0.1676, -0.2469, -0.3393,  0.2286],\n",
      "        [ 0.3451, -0.1171, -0.5045,  0.8785,  0.0982,  0.4897,  0.3418,  0.7972,\n",
      "          0.7744, -0.4583,  0.2891, -0.0062, -0.3099, -0.4260, -0.0556,  0.1314,\n",
      "         -0.1138, -0.4651,  0.2821,  0.0896]], grad_fn=<AddmmBackward0>)\n",
      "After ReLU: tensor([[0.5655, 0.1406, 0.0000, 0.4963, 0.2198, 0.6241, 0.2940, 0.8591, 0.5996,\n",
      "         0.0000, 0.2359, 0.0000, 0.0000, 0.0000, 0.0269, 0.0130, 0.0000, 0.1911,\n",
      "         0.0000, 0.4135],\n",
      "        [0.6640, 0.2588, 0.0000, 0.5884, 0.6104, 0.2786, 0.0273, 0.9439, 0.5719,\n",
      "         0.0000, 0.1015, 0.2043, 0.0000, 0.0000, 0.1580, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2286],\n",
      "        [0.3451, 0.0000, 0.0000, 0.8785, 0.0982, 0.4897, 0.3418, 0.7972, 0.7744,\n",
      "         0.0000, 0.2891, 0.0000, 0.0000, 0.0000, 0.0000, 0.1314, 0.0000, 0.0000,\n",
      "         0.2821, 0.0896]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\")\n",
    "hidden2 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_models = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "input_image=torch.rand(3,28,28)\n",
    "logits = seq_models(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_prob = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0213, -0.0229,  0.0112,  ...,  0.0219, -0.0218,  0.0167],\n",
      "        [-0.0230, -0.0085,  0.0047,  ...,  0.0181,  0.0347, -0.0034]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0299, 0.0051], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0203, -0.0351,  0.0419,  ..., -0.0061, -0.0417,  0.0018],\n",
      "        [-0.0284,  0.0274, -0.0202,  ...,  0.0219, -0.0100, -0.0256]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0070, -0.0177], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0182, -0.0319, -0.0066,  ...,  0.0294,  0.0281,  0.0327],\n",
      "        [-0.0276,  0.0249, -0.0157,  ...,  0.0015, -0.0059, -0.0314]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0134, -0.0411], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Structure: \",model,\"\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f36856cb0c9c163074b8ccbff3cb908830e02bd61305bc98cc83e2e0ff3f3ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
